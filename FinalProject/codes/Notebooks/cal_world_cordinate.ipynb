{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load calibration data\n",
    "calibration_file = 'cam_calib\\camera_calibration.npz'\n",
    "calibration_data = np.load(calibration_file)\n",
    "\n",
    "# Extract calibration parameters\n",
    "camera_matrix = calibration_data['camera_matrix']\n",
    "dist_coeffs = calibration_data['dist_coeffs']\n",
    "rvecs = calibration_data['rvecs']  # Rotation vectors\n",
    "tvecs = calibration_data['tvecs']  # Translation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.        ]\n",
      " [ 0.          0.99120281  0.13235175]\n",
      " [ 0.         -0.13235175  0.99120281]]\n",
      "asdasdasd\n",
      "Camera position in world coordinates: [  0.66       -55.96823558 -15.16211133]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "drone_world = np.array([0.66, -56, -15.4])  # Replace x, y, z with the drone's world coordinates\n",
    "camera_relative = np.array([0, 0, 0.24])\n",
    "pitch_angle_deg = -25  # Negative for downward tilt\n",
    "pitch_angle_rad = np.radians(pitch_angle_deg)\n",
    "\n",
    "# Rotation matrix for pitch\n",
    "R_x = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, np.cos(pitch_angle_rad), -np.sin(pitch_angle_rad)],\n",
    "    [0, np.sin(pitch_angle_rad), np.cos(pitch_angle_rad)]\n",
    "])\n",
    "\n",
    "\n",
    "print(R_x)\n",
    "print(\"\")\n",
    "print(camera_relative)\n",
    "# Rotate camera position\n",
    "camera_relative_world = R_x @ camera_relative\n",
    "\n",
    "# Calculate camera position in world coordinates\n",
    "camera_world = drone_world + camera_relative_world\n",
    "\n",
    "print(\"Camera position in world coordinates:\", camera_world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_object_location(bbox, depth_image, intrinsic_matrix, camera_position, camera_orientation):\n",
    "    \"\"\"\n",
    "    Calculate the 3D coordinates of an object in world coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        bbox (tuple): Bounding box of the object in the format (x_min, y_min, x_max, y_max).\n",
    "        depth_image (numpy.ndarray): Depth image with depth values in meters.\n",
    "        intrinsic_matrix (numpy.ndarray): 3x3 camera intrinsic matrix.\n",
    "        camera_position (numpy.ndarray): 3x1 vector representing the camera position in world coordinates.\n",
    "        camera_orientation (numpy.ndarray): 3x3 rotation matrix representing the camera orientation in world coordinates.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 3D coordinates (x, y, z) of the object in world coordinates.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # Extract the depth value at the center of the bounding box\n",
    "    u_center = int((x_min + x_max) / 2)\n",
    "    v_center = int((y_min + y_max) / 2)\n",
    "    depth = depth_image[v_center, u_center]\n",
    "\n",
    "    print(depth)\n",
    "    if depth <= 0 or depth > 80:\n",
    "        raise ValueError(\"Invalid depth value: {}. Ensure depth is between 0 and 80 meters.\".format(depth))\n",
    "\n",
    "    # Camera intrinsic parameters\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "\n",
    "    # Convert pixel coordinates to normalized image coordinates\n",
    "    x_norm = (u_center - cx) / fx\n",
    "    y_norm = (v_center - cy) / fy\n",
    "\n",
    "    # Convert to 3D coordinates in the camera frame\n",
    "    camera_coords = np.array([x_norm * depth, y_norm * depth, depth])\n",
    "\n",
    "    # Transform to world coordinates\n",
    "    world_coords = camera_orientation @ camera_coords + camera_position\n",
    "\n",
    "    return world_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "Object location in world coordinates: x=-52.63688505263953, y=3.2709880027594482, z=-9.695\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "bounding_box = [136,259,205, 324]  # Example bounding box\n",
    "depth_image = Image.open(\"depth.jpg\")   # Example depth image\n",
    "depth_array = np.array(depth_image)[:,:,0]*80/256 # detected range is between 0 to 80 meter, 0 for nearest depth and 80 for farset depth\n",
    "\n",
    "intrinsic_matrix = camera_matrix # Example intrinsic matrix\n",
    "camera_position = np.array([-51.87, 2.49, -10.57])  # Camera position in world coordinates\n",
    "camera_orientation = np.eye(3)  # Example: identity matrix (no rotation)\n",
    "\n",
    "# Calculate object location\n",
    "x, y, z = calculate_object_location(bounding_box, depth_array, intrinsic_matrix, camera_position, camera_orientation)\n",
    "print(f\"Object location in world coordinates: x={x}, y={y}, z={z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'ts': 1737812989094138368, \n",
    "'position': [0.6599999666213989, -56.0, -15.401533126831055], \n",
    "'orientation': <scipy.spatial.transform._rotation.Rotation object at 0x0000029D2280D6B0>, \n",
    "'linear_velocity': [3.789719755786791e-07, -2.4927132358243398e-08, -0.00014954849029891193], \n",
    "'angular_velocity': [-5.281396522373305e-13, 2.223909518761502e-07, -1.3892886840949359e-08], \n",
    "'linear_acceleration': [2.5453998020452673e-08, -1.821736539397989e-09, 0.0], \n",
    "'has_collided': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------[]\n",
      "0.0\n",
      "0.9375\n",
      "----------[]\n",
      "Nearest Distance: 0.0\n",
      "Farthest Distance: 0.9375\n",
      "----------[]\n",
      "World Coordinates: [[-17.96234915 -89.96234915 -22.96234915]\n",
      " [-17.89189228 -89.89189228 -22.89189228]\n",
      " [-17.875      -89.875      -22.875     ]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Example bounding box and depth image\n",
    "\n",
    "  # x_min, y_min, x_max, y_max\n",
    "camera_position = np.array([0.6599999666213989, -56.0, -15.401533126831055])  # Camera position in world coordinates\n",
    "depth_image =  Image.open(\"depth.jpg\") \n",
    "depth_array = np.array(depth_image)\n",
    "\n",
    "nearest_distance, farthest_distance, world_coords = compute_object_location(\n",
    "    depth_image=depth_array,\n",
    "    bbox=bounding_box,\n",
    "    camera_matrix=camera_matrix,\n",
    "    rotation_matrix=rotation_matrix,\n",
    "    camera_position=camera_position\n",
    ")\n",
    "\n",
    "print(f\"Nearest Distance: {nearest_distance}\")\n",
    "print(f\"Farthest Distance: {farthest_distance}\")\n",
    "print(\"----------[]\")\n",
    "print(f\"World Coordinates: {world_coords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_calibration_data(calibration_file):\n",
    "    \"\"\"\n",
    "    Load camera calibration parameters\n",
    "\n",
    "    Parameters:\n",
    "        calibration_file \n",
    "\n",
    "    Returns:\n",
    "        dict: camera_matrix، dist_coeffs، rvecs، tvecs\n",
    "    \"\"\"\n",
    "    calibration = np.load(calibration_file)\n",
    "    calibration_data = {\n",
    "        'camera_matrix': calibration['camera_matrix'],\n",
    "        'dist_coeffs': calibration['dist_coeffs'],\n",
    "        'rvecs': calibration['rvecs'],\n",
    "        'tvecs': calibration['tvecs']\n",
    "    }\n",
    "    return calibration_data\n",
    "\n",
    "def compute_camera_world_position(drone_world, camera_relative, pitch_angle_deg):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        drone_world (numpy.ndarray)\n",
    "        camera_relative (numpy.ndarray)\n",
    "        pitch_angle_deg (float)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: drone state in world cordinate \n",
    "        numpy.ndarray: camera rotate matrix related to drone\n",
    "    \"\"\"\n",
    "    pitch_angle_rad = np.radians(pitch_angle_deg)\n",
    "\n",
    "    # Pitch rotation in X direction\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(pitch_angle_rad), -np.sin(pitch_angle_rad)],\n",
    "        [0, np.sin(pitch_angle_rad), np.cos(pitch_angle_rad)]\n",
    "    ])\n",
    "\n",
    "    # convert camera cordinate to world cordinate\n",
    "    camera_relative_world = R_x @ camera_relative\n",
    "    camera_world = drone_world + camera_relative_world\n",
    "\n",
    "    return camera_world, R_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_obstacles(depth_img, threshold=1.0):\n",
    "    \"\"\"\n",
    "    search to near ocl\n",
    "\n",
    "    Parameters:\n",
    "        depth_img\n",
    "        threshold\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "\n",
    "    if np.any(depth_img < threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(model, undistorted_img):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        model\n",
    "        undistorted_img\n",
    "\n",
    "    Returns:\n",
    "        list of detected objects bbx\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        detections = model.detect(undistorted_img)\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_depth_image(depth_image_path, scale_factor=80/256):\n",
    "    \"\"\"\n",
    "    process on depth image like scaling and etc.\n",
    "\n",
    "    Parameters:\n",
    "        depth_image_path \n",
    "        scale_factor  \n",
    "    \"\"\"\n",
    "    \n",
    "    depth_image = Image.open(depth_image_path)\n",
    "    depth_array = np.array(depth_image)[:, :, 0] * scale_factor  # detected range is between 0 to 80 meters\n",
    "    return depth_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_object_location(bbox, depth_image, intrinsic_matrix, camera_position, camera_orientation):\n",
    "    \"\"\"\n",
    "    calculate object location in world cordinate\n",
    "\n",
    "    Parameters:\n",
    "        bbox : (x_min, y_min, x_max, y_max)\n",
    "        depth_image : depth image that scaled to 0 to 80 meters\n",
    "        intrinsic_matrix : camera matrix\n",
    "        camera_position \n",
    "        camera_orientation\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 3d cordinate of detected objects\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # extract depth centera of detected object\n",
    "    u_center = int((x_min + x_max) / 2)\n",
    "    v_center = int((y_min + y_max) / 2)\n",
    "    depth = depth_image[v_center, u_center]\n",
    "\n",
    "    print(f\"Depth at center: {depth} meters\")\n",
    "    if depth <= 0 or depth > 80:\n",
    "        raise ValueError(f\"Invalid depth value: {depth}. Ensure depth is between 0 and 80 meters.\")\n",
    "\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "\n",
    "    # Normalized pixel cordinate\n",
    "    x_norm = (u_center - cx) / fx\n",
    "    y_norm = (v_center - cy) / fy\n",
    "\n",
    "    camera_coords = np.array([x_norm * depth, y_norm * depth, depth])\n",
    "\n",
    "    world_coords = camera_orientation @ camera_coords + camera_position\n",
    "\n",
    "    return world_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sim.get_drone_state()\n",
    "\n",
    "qx, qy, qz, qw = state['qx'], state['qy'], state['qz'], state['qw']\n",
    "rotation_body = R.from_quat([qx, qy, qz, qw])\n",
    "R_wb = rotation_body.as_matrix()\n",
    "\n",
    "camera_orientation = R_wb @ R_bc\n",
    "\n",
    "world_coords = calculate_object_location(\n",
    "    bbox=det['bbox'],\n",
    "    depth_image=depth_array,\n",
    "    intrinsic_matrix=camera_matrix,\n",
    "    camera_position=camera_world,\n",
    "    camera_orientation=camera_orientation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
